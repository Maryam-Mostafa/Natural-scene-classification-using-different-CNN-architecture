{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalSceneClassification_CNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maryam-Mostafa/Natural-scene-classification-using-different-CNN-architecture/blob/master/NaturalSceneClassification_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3GRIqAz_7EL"
      },
      "source": [
        "### **Liberaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlU8xrTBIzO8"
      },
      "source": [
        "!pip install tflearn\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import matplotlib.pyplot as plt\n",
        "import tflearn\n",
        "from keras.utils import np_utils\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiNGthrWAIVQ"
      },
      "source": [
        "### **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS7MJRN9JAeW"
      },
      "source": [
        "def Read_Data(path_,label,isTrain):\n",
        "    sub_data = []\n",
        "    for img in tqdm(os.listdir(path_)):\n",
        "        path = os.path.join(path_, img)\n",
        "        img_data = cv2.imread(path, 0)\n",
        "       img_data = cv2.resize(img_data, (150, 150)) \n",
        "        if isTrain:\n",
        "          sub_data.append([np.array(img_data), label])\n",
        "        else:\n",
        "          sub_data.append([np.array(img_data), img])\n",
        "    return sub_data\n",
        "\n",
        "def Creat_all_train_data():\n",
        "    training_data,sub1,sub2,sub3,sub4,sub5,sub6 = [],[],[],[],[],[],[]\n",
        "          \n",
        "    sub1 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/buildings/\",0,True)\n",
        "    sub2 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/forest\",1,True)\n",
        "    sub3 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/glacier\",2,True)\n",
        "    sub4 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/mountain\",3,True)\n",
        "    sub5 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/sea\",4,True)\n",
        "    sub6 = Read_Data(\"/content/drive/MyDrive/NNP/Scenetrainingset/street\",5,True)\n",
        "    training_data = sub1 + sub2 + sub3 + sub4 + sub5 + sub6\n",
        "    shuffle(training_data)\n",
        "    np.save('/content/drive/MyDrive/NNP/train_dataset.npy', training_data)\n",
        "\n",
        "    return training_data\n",
        "\n",
        "if (os.path.exists('/content/drive/MyDrive/NNP/train_dataset.npy')): \n",
        "    train =np.load('/content/drive/MyDrive/NNP/train_dataset.npy',allow_pickle=True)\n",
        "\n",
        "else: \n",
        "    train = Creat_all_train_data()\n",
        "\n",
        "if (os.path.exists('/content/drive/MyDrive/NNP/test_dataset.npy')): \n",
        "    test =np.load('/content/drive/MyDrive/NNP/test_dataset.npy',allow_pickle=True)\n",
        "\n",
        "else: \n",
        "      test = Read_Data(\"/content/drive/MyDrive/NNP/Scenestestingtest\",None,False)\n",
        "      np.save('/content/drive/MyDrive/NNP/test_dataset.npy', test)\n",
        "\n",
        "X_train = np.array([i[0] for i in train]).reshape(-1, 150, 150, 1)\n",
        "Y_train = np.array([i[1] for i in train])\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes=6)\n",
        "\n",
        "\n",
        "X_test = np.array([i[0] for i in test]).reshape(-1, 150, 150, 1)\n",
        "img_name = np.array([i[1] for i in test])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwMYUBBWAuDx"
      },
      "source": [
        "### **Model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRnuWvphAfv7"
      },
      "source": [
        "ops.reset_default_graph()\n",
        "conv_input = input_data(shape=[None, 150, 150, 1], name='input')\n",
        "conv1 = conv_2d(conv_input, 32, 5, activation='relu')\n",
        "pool1 = max_pool_2d(conv1, 5)\n",
        "\n",
        "conv2 = conv_2d(pool1, 64, 5, activation='relu')\n",
        "pool2 = max_pool_2d(conv2, 5)\n",
        "\n",
        "conv3 = conv_2d(pool2, 128, 5, activation='relu')\n",
        "pool3 = max_pool_2d(conv3, 5)\n",
        "\n",
        "conv4 = conv_2d(pool3, 64, 5, activation='relu')\n",
        "pool4 = max_pool_2d(conv4, 5)\n",
        "\n",
        "conv5 = conv_2d(pool4, 32, 5, activation='relu')\n",
        "pool5 = max_pool_2d(conv5, 5)\n",
        "\n",
        "conv6 = conv_2d(pool5, 64, 5, activation='relu')\n",
        "pool6 = max_pool_2d(conv6, 5)\n",
        "\n",
        "conv7 = conv_2d(pool6, 32, 5, activation='relu')\n",
        "pool7 = max_pool_2d(conv7, 5)\n",
        "\n",
        "conv8 = conv_2d(pool7, 32, 5, activation='relu')\n",
        "pool8 = max_pool_2d(conv8, 5)\n",
        "\n",
        "fully_layer = fully_connected(pool8, 1500, activation='relu')\n",
        "fully_layer = dropout(fully_layer, 0.5)\n",
        "\n",
        "cnn_layers = fully_connected(fully_layer, 6, activation='softmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3vpHLZPBC7k"
      },
      "source": [
        "### **Training the model and geting prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W81Enj3cBB3Q"
      },
      "source": [
        "cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy', name='targets')\n",
        "model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n",
        "model.fit( X_train, Y_train, n_epoch=40,snapshot_step=100, show_metric=True, run_id='cnn')\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "prediction = np.argmax(prediction, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOt3UjbIAjJ6"
      },
      "source": [
        "### **saving the result in csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu4gjVc0AiEs"
      },
      "source": [
        "rows = {'Image': [],'Label': []}\n",
        "csvFile = pd.DataFrame(rows, columns = ['Image', 'Label'])\n",
        "\n",
        "csvFile[\"Image\"] = img_name\n",
        "csvFile[\"Label\"] = np.asarray(prediction)\n",
        "\n",
        "csvFile.to_csv('submit2.csv')\n",
        "files.download('submit2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}